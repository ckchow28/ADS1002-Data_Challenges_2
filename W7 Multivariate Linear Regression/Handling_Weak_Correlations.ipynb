{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef25fdbc-2a7f-4610-a4c2-32fcc827fbf3",
   "metadata": {},
   "source": [
    "#### Background\n",
    "\n",
    "If all the independent variables in your correlation analysis have a correlation coefficient of less than `0.5` with the dependent variable, it generally suggests that the individual linear relationships between each independent variable and the dependent variable are weak. Here’s what you can do in this situation:\n",
    "\n",
    "1. **Increase the Data**\n",
    "\n",
    "  - **Collect More Data**: A larger dataset might help in better understanding the relationships between variables and could reveal stronger correlations.\n",
    "\n",
    "2. **Re-evaluate the Choice of Variables**\n",
    "\n",
    "   - **Reassess the Variables**: Sometimes, the chosen independent variables might not be the best predictors of the dependent variable. Consider adding or replacing some of them with variables that might have a stronger theoretical basis for being related to the dependent variable.\n",
    "  \n",
    "3. **Increase the Complexity of the Model**:\n",
    "\n",
    "    - **Multivariate Linear Regression**: Even if individual variables have a low correlation with the dependent variable, a combination of them might explain a significant portion of the variance in the dependent variable. Multivariate linear regression can help in capturing this combined effect.\n",
    "    - **Regularization Techniques**: Techniques like Ridge or Lasso regression can help in handling cases where the relationship between the variables is complex and might not be apparent from simple correlation analysis.\n",
    "  \n",
    "**Example in Python**:\n",
    "\n",
    "Here’s a simple example using a synthetic dataset where the correlations are initially weak, but through variable transformations and combining variables, we can improve the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8a98271-5475-4ec8-b5e1-3189a001aeea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlations with 'y':\n",
      " X1    0.284059\n",
      "X2    0.516202\n",
      "X3    0.219343\n",
      "Name: y, dtype: float64\n",
      "\n",
      "Variables with correlation less than 0.5 with 'y': ['X1', 'X3']\n",
      "\n",
      "R-squared score using 'X1' alone: 0.04198987471794191\n",
      "\n",
      "R-squared score using 'X3' alone: -0.2526691785910511\n",
      "\n",
      "R-squared score using multivariate linear regression on low correlated variables: 0.15745578814428884\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Creating a synthetic dataset\n",
    "np.random.seed(0)\n",
    "X1 = np.random.rand(100)\n",
    "X2 = np.random.rand(100)\n",
    "X3 = np.random.rand(100)\n",
    "# Dependent variable with weak linear relationships\n",
    "y = 0.2 * X1 + 0.3 * X2 + 0.1 * X3 + np.random.randn(100) * 0.1\n",
    "\n",
    "# Creating a DataFrame\n",
    "df = pd.DataFrame({'X1': X1, 'X2': X2, 'X3': X3, 'y': y})\n",
    "\n",
    "# Calculating correlation with the dependent variable\n",
    "correlations = df.corr()['y'].drop('y')\n",
    "print(\"Correlations with 'y':\\n\", correlations)\n",
    "\n",
    "# Identifying variables with correlation less than 0.5\n",
    "low_corr_vars = correlations[correlations < 0.5].index.tolist()\n",
    "print(\"\\nVariables with correlation less than 0.5 with 'y':\", low_corr_vars)\n",
    "\n",
    "# Evaluating individual variable performance\n",
    "for var in low_corr_vars:\n",
    "    X_single = df[[var]]\n",
    "    X_train_single, X_test_single, y_train_single, y_test_single = train_test_split(X_single, y, test_size=0.2, random_state=0)\n",
    "    \n",
    "    model_single = LinearRegression()\n",
    "    model_single.fit(X_train_single, y_train_single)\n",
    "    y_pred_single = model_single.predict(X_test_single)\n",
    "    \n",
    "    print(f\"\\nR-squared score using '{var}' alone: {r2_score(y_test_single, y_pred_single)}\")\n",
    "\n",
    "# Using all the low correlated variables for multivariate regression\n",
    "X = df[low_corr_vars]\n",
    "\n",
    "# Splitting the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Fitting a multivariate linear regression model\n",
    "model_multi = LinearRegression()\n",
    "model_multi.fit(X_train, y_train)\n",
    "\n",
    "# Predicting and evaluating the multivariate model\n",
    "y_pred_multi = model_multi.predict(X_test)\n",
    "print(\"\\nR-squared score using multivariate linear regression on low correlated variables:\", r2_score(y_test, y_pred_multi))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b93c9e-a76f-4ef3-9dad-1fb1c28aad28",
   "metadata": {},
   "source": [
    "### Code Explanation\n",
    "\n",
    "1. **Correlation Check:**\n",
    "   - The script calculates and displays the correlation of each independent variable (`X1`, `X2`, `X3`) with the dependent variable (`y`).\n",
    "2. **Identifying Low-Correlation Variables::**\n",
    "   - It identifies variables that have a correlation of less than 0.5 with `y` and stores them in `low_corr_vars`.\n",
    "3. **Individual Variable Performance:**\n",
    "    - For each variable with low correlation, the script fits a simple linear regression model using just that variable.\n",
    "    - The R-squared score is calculated and printed to show how well each variable, when used alone, predicts the dependent variable.\n",
    "4. **Multivariate Linear Regression:**\n",
    "   - The script then fits a multivariate linear regression model using all the low-correlated variables together.\n",
    "   - It calculates and prints the R-squared score for this model, demonstrating the combined effect of the variables on the prediction.\n",
    "  \n",
    "**Output**\n",
    "- **R-squared Scores:**\n",
    "   - The output will first show the individual R-squared scores for each low-correlated variable.\n",
    "   - Finally, it will show the R-squared score for the multivariate regression, which is expected to be higher, illustrating the improvement in prediction when using the variables together."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6678155c-a88d-40bc-ae1c-dd076ed3855c",
   "metadata": {},
   "source": [
    "### Understanding R-squared\n",
    "\n",
    "R-squared, also known as the coefficient of determination, is a statistical measure that shows how well the independent variables in a model explain the variability of the dependent variable. In simple terms, it tells you how much of the change in the dependent variable (what you’re trying to predict) can be explained by the independent variables (the inputs).\n",
    "\n",
    "#### Simple Explanation\n",
    "\n",
    "- **R-squared Value:** The value of R-squared ranges from `0` to `1` (or 0% to 100% when expressed as a percentage).\n",
    "  - **0** means that the independent variables do not explain any of the variation in the dependent variable. In other words, the model is not useful at all.\n",
    "  - **1** (or 100%) means that the independent variables explain all the variation in the dependent variable. The model perfectly predicts the outcome.\n",
    "  - **0.7** (or 70%) means that 70% of the variation in the dependent variable can be explained by the independent variables.\n",
    " \n",
    "#### Example\n",
    "\n",
    "Let's say you're a teacher trying to predict students' final exam scores based on the number of hours they studied.\n",
    "\n",
    "- **Case 1:** You find that the number of study hours alone gives you an R-squared value of 0.5 (or 50%). This means that 50% of the variation in exam scores can be explained by the number of study hours. The other 50% is due to factors that the model does not account for, such as student motivation, prior knowledge, or exam difficulty.\n",
    "- **Case 2:** You then include additional factors like class attendance and participation in your model, and the R-squared value increases to 0.8 (or 80%). This means that by considering these additional factors, 80% of the variation in exam scores can now be explained by the model. The model is more accurate in predicting the exam scores because it takes more relevant factors into account.\n",
    "\n",
    "In summary, R-squared gives you a quick sense of how well your model is doing at predicting the dependent variable based on the independent variables you've chosen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed02f674-e51b-4a83-8e33-10df258b431c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
